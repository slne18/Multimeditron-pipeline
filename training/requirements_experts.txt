transformers==4.45.0
flash-attn<=2.4.2 # Check https://github.com/Dao-AILab/flash-attention/issues/867
einops
monai==1.3.2
open_clip_torch
deepspeed==0.15.1
clip==0.2.0
pillow>=9.4.0
